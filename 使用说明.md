# 文本相似度模型训练系统使用说明

## 📁 项目结构

```
audio_supportor/
├── core/                    # 核心模块
│   ├── models/             # 模型定义
│   │   ├── __init__.py
│   │   ├── tfidf_lr_model.py   # TF-IDF + LR 模型类
│   │   ├── bert_model.py       # BERT 模型类
│   │   └── lstm_model.py       # Word2Vec + LSTM 模型类
│   ├── train/              # 训练脚本
│   │   ├── __init__.py
│   │   ├── tfidf_lr_train.py   # TF-IDF + LR 模型训练
│   │   ├── bert_train.py       # BERT 模型训练
│   │   └── lstm_train.py       # Word2Vec + LSTM 模型训练
│   ├── utils/              # 工具模块
│   │   ├── __init__.py
│   │   ├── config.py          # 配置管理
│   │   └── logger.py          # 日志管理
│   ├── dependencies/        # 依赖模块
│   │   ├── __init__.py
│   │   └── globals.py         # 全局变量
│   ├── __init__.py
│   └── config.yaml         # 配置文件
├── data/                   # 数据目录
│   └── round1/            # 第一轮数据
│       ├── gaiic_track3_round1_train_20210228.tsv
│       └── gaiic_track3_round1_testA_20210228.tsv
├── results/               # 结果输出目录
├── logs/                  # 日志目录
├── requirements.txt       # Python依赖包
├── requirements_cpu.txt   # CPU版本依赖包
├── requirements_gpu.txt   # GPU版本依赖包
├── install_dependencies.py # 自动安装脚本
├── run_training.py        # 主训练脚本
└── README_enhanced.md     # 技术文档
```

## 🚀 快速开始

### 0. 目录结构说明

项目采用了模块化的设计：
- `core/models/` - 包含所有模型的定义类（BertModelWrapper、LstmModelWrapper、TfidfLRModelWrapper）
- `core/train/` - 训练脚本，调用对应的模型类
- `core/utils/` - 工具模块，包含配置管理和日志管理
- `core/dependencies/` - 全局依赖和变量
- `data/` - 数据文件目录
- `results/` - 结果输出目录
- `logs/` - 日志输出目录

这种设计使得：
- 模型定义和训练逻辑分离，便于维护
- 可以轻松添加新的模型
- 代码复用性更好
- 配置集中管理，便于调参

### 1. 环境准备

#### 方式一：自动安装（推荐）

运行自动安装脚本，它会检测您的环境并选择合适的版本：

```bash
python install_dependencies.py
```

#### 方式二：手动安装

根据您的环境选择合适的版本：

**CPU版本（推荐用于快速测试）：**
```bash
pip install -r requirements_cpu.txt
```

**GPU版本（推荐用于完整训练）：**
```bash
pip install -r requirements_gpu.txt
```

**通用版本（自动检测GPU）：**
```bash
pip install -r requirements.txt
```

**注意：** GPU版本需要先安装CUDA和cuDNN。

### 2. 快速测试

在运行完整训练之前，建议先运行快速测试脚本验证环境配置：

```bash
python test_config.py
```

这个脚本会：
- 测试配置加载功能
- 验证所有依赖是否正确安装
- 检查数据类型转换是否正确
- 输出测试结果

如果配置加载成功，说明环境配置正确。

### 3. 运行训练

#### 方式一：一键运行所有模型（推荐）

```bash
python run_training.py
```

这将依次训练：
1. TF-IDF + LR 模型
2. BERT 微调模型  
3. Word2Vec + LSTM 模型

#### 方式二：单独运行某个模型

```bash
# 只训练TF-IDF + LR
python -m core.train.tfidf_lr_train

# 只训练BERT
python -m core.train.bert_train

# 只训练Word2Vec + LSTM
python -m core.train.lstm_train
```

#### 方式三：直接运行模型类

```bash
# 直接运行BERT模型
python core/models/bert_model.py

# 直接运行LSTM模型
python core/models/lstm_model.py

# 直接运行TF-IDF + LR模型
python core/models/tfidf_lr_model.py
```

## 📊 输出文件

训练完成后会在 `results/` 目录下生成：

### 单个模型结果
- `tfidf_lr_result.csv` - TF-IDF + LR 预测结果
- `bert_result.csv` - BERT 预测结果
- `lstm_result.csv` - Word2Vec + LSTM 预测结果

### 模型文件
- `word2vec_model.model` - 训练好的Word2Vec模型

### 日志文件
- `logs/audio_supporter.log` - 训练日志

## 🎯 模型特点对比

| 模型 | 训练速度 | 内存需求 | 性能预期 | 适用场景 |
|------|----------|----------|----------|----------|
| TF-IDF + LR | 很快 | 低 | 中等 | 快速baseline，大数据集 |
| BERT | 很慢 | 高 | 高 | 小数据集，需要强语义理解 |
| Word2Vec + LSTM | 中等 | 中等 | 中高 | 中等数据集，平衡方案 |

## 🔧 高级用法

### 自定义配置

修改 `core/config.yaml` 文件来调整模型参数：

```yaml
# BERT 模型配置
bert:
  model_name: "bert-base-chinese"
  max_len: 128
  batch_size: 16
  epochs: 2
  learning_rate: 2e-5
  dropout: 0.3
  num_classes: 2

# LSTM 模型配置
lstm:
  embedding_dim: 100
  hidden_dim: 128
  num_layers: 2
  max_len: 50
  batch_size: 32
  epochs: 5
  learning_rate: 0.001
  dropout: 0.3
  bidirectional: true
  num_classes: 2
```

### 修改训练参数

在 `core/config.yaml` 中修改训练相关参数：

```yaml
# 训练配置
training:
  n_folds: 5
  random_state: 2020
  shuffle: true
  results_dir: "../../results"
```

## ⚙️ 参数调优

### TF-IDF + LR 参数
在 `core/config.yaml` 中修改：
- `ngram_range`: n-gram范围
- `n_components`: SVD降维维度
- `C`: 逻辑回归正则化参数

### BERT 参数
在 `core/config.yaml` 中修改：
- `max_len`: 文本最大长度
- `batch_size`: 批次大小
- `epochs`: 训练轮数
- `learning_rate`: 学习率

### LSTM 参数
在 `core/config.yaml` 中修改：
- `embedding_dim`: 词向量维度
- `hidden_dim`: LSTM隐藏层维度
- `num_layers`: LSTM层数
- `max_len`: 序列最大长度

## ❓ 常见问题

### Q1: 训练时间太长怎么办？

**A:** 可以：
- 减少交叉验证折数（修改 `n_folds` 参数）
- 减少训练轮数（修改 `epochs` 参数）
- 减小批次大小（修改 `batch_size` 参数）
- 只运行轻量级模型（TF-IDF + LR）

### Q2: 内存不足怎么办？

**A:** 可以尝试：
- 减小 `max_len` 参数
- 减小 `batch_size`
- 使用CPU训练（修改device设置）
- 只运行轻量级模型（TF-IDF + LR）

### Q3: 如何只运行部分模型？

**A:** 使用单独的训练脚本：
```bash
# 只运行TF-IDF + LR
python -m core.train.tfidf_lr_train

# 只运行BERT
python -m core.train.bert_train

# 只运行LSTM
python -m core.train.lstm_train
```

### Q4: 如何查看训练进度？

**A:** 每个脚本都会显示详细的训练进度，包括：
- 当前fold进度
- 每个epoch的loss和AUC
- 最终的平均性能
- 日志文件保存在 `logs/` 目录

### Q5: 配置修改后不生效？

**A:** 检查以下几点：
- 确保修改的是 `core/config.yaml` 文件
- 重启Python进程以重新加载配置
- 检查配置文件的YAML格式是否正确

## 🔍 故障排除

### 错误1: CUDA out of memory
```
解决方案：减小batch_size或max_len
```

### 错误2: ModuleNotFoundError
```
解决方案：运行 pip install -r requirements.txt
```

### 错误3: 数据文件路径错误
```
解决方案：确保数据文件在 data/round1/ 目录下
```

### 错误4: BERT模型下载失败
```
解决方案：检查网络，或使用本地模型
```

### 错误5: 学习率类型错误
```
解决方案：确保配置文件中的learning_rate是数值类型，不是字符串
```

### 错误6: KeyError: 'tokens'
```
解决方案：检查LSTM模型的数据处理流程，确保分词正确
```

## 📈 性能优化建议

### 1. 数据预处理优化
- 添加文本清洗步骤
- 使用更好的分词工具
- 添加同义词替换等数据增强

### 2. 模型优化
- 使用更先进的预训练模型（RoBERTa、ALBERT等）
- 尝试不同的网络架构（BERT-CNN、BERT-LSTM等）
- 使用学习率调度器

### 3. 配置优化
- 根据硬件配置调整batch_size
- 根据数据特点调整max_len
- 使用交叉验证找到最佳超参数

